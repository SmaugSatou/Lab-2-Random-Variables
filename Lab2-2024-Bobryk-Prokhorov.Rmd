---
title: 'P&S-2022: Lab assignment 2'
author: "Vladyslav Bobryk, Roman Prokhorov"
output:
  html_document:
    df_print: paged
---

## Introduction

### Team Information

**Team ID:** 31

```{r}
id <- 31
set.seed(31)
```

### Work Distribution

-   **Vladyslav Bobryk**:
    -   Task 2: Poisson distribution and radioactive decay
    -   Task 3: Central Limit Theorem for exponential distribution
    -   Contribution: \~50%
-   **Roman Prokhorov**:
    -   Task 1: Hamming code implementation and analysis
    -   Task 4: Independence and moments of random variables
    -   Contribution: \~50%

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$
    G :=
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
p <- id / 100
N <- 100
p_star <- (1 - p)^7 + 7 * p * (1 - p)^6


# Matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))
cat("The matrix G is: \n")
G
cat("The matrix H is: \n")
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
# Generate N messages
message_generator <- function(N) {
  matrix(sample(c(0, 1), 4 * N, replace = TRUE), nrow = N)
}

messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
# Generate errors
errors <- matrix(rbinom(N * 7, size = 1, prob = p), nrow = N)

# Apply errors to codewords to get received messages
# XOR operation
received <- (codewords + errors) %% 2
```

#### 1.Detecting and Correcting Errors

```{r}
# Calculate syndrome vector
syndrome_vector <- (received %*% H) %% 2

# Function to convert syndrome (3 bits) to error position (1-7)
decode_syndrome <- function(syndrome) {
    syndrome[1] + 2 * syndrome[2] + 4 * syndrome[3]
}

corrected <- received

for (i in 1:N) {
    # Find which bit position has error
    error_idx <- decode_syndrome(syndrome_vector[i, ])

    # If error is in position 1-7, flip bit
    if (error_idx > 0 && error_idx <= 7) {
        corrected[i, error_idx] <- (corrected[i, error_idx] + 1) %% 2
    }
}

# Extract the original 4-bit message
decoded <- corrected[, c(3, 5, 6, 7)]

# Check which messages were transmitted correctly
success <- apply(decoded == messages, 1, all)

# Estimate probability of correct transmission
p_hat <- mean(success)

p_star
p_hat
```

**Comment on why $\hat p$ is close to $p^*$ for large $N$:** The estimate $\hat p$ is the sample mean of the success indicator, which takes value 1 when a message is transmitted correctly and 0 otherwise.
By the **Law of Large Numbers (LLN)**, as the number of simulations $N$ increases, the sample mean $\hat p$ converges to the true probability $p^*$.
This happens because each transmission trial is independent and has the same probability $p^*$ of success.
In simple terms, when we repeat the experiment many times, the proportion of successes gets closer and closer to the true success probability.
The more simulations we run, the more reliable our estimate becomes, which is why we can be confident that $\hat p \approx p^*$ when $N$ is large.
With $N=100$, we got $\hat p = 0.27$, which is reasonably close to the theoretical $p^* = 0.309$, and the difference would decrease with larger $N$.

#### 2.Confidence Interval
```{r}
# Calculate standard error
standard_error <- sd(success) / sqrt(N)

# For 95% confidence interval we use z = 1.96
# This comes from the normal distribution: P(-1.96 < Z < 1.96) = 0.95
z_score <- qnorm(0.975)  # 0.975 because we want 2.5% in each tail

# Calculate margin of error
epsilon <- z_score * standard_error

# Confidence interval bounds
ci_lower <- p_hat - epsilon
ci_upper <- p_hat + epsilon

cat("95% Confidence Interval: [", ci_lower, ",", ci_upper, "]\n")
cat("Margin of error (Îµ):", epsilon, "\n")
```

**Comment:** Using the **Central Limit Theorem (CLT)**, we know that for large $N$, the distribution of $\hat p$ is approximately normal with mean $p^*$ and standard deviation $\sigma/\sqrt{N}$, where $\sigma$ is the standard deviation of the success indicator.
We estimate $\sigma$ using the sample standard deviation.
The confidence interval $[0.183, 0.357]$ tells us that we can be 95% confident that the true probability $p^*$ lies within this range.
Indeed, our theoretical $p^* = 0.309$ falls within this interval, confirming our method works correctly.
The margin of error $\varepsilon = 0.087$ decreases as $N$ increases, giving us more precise estimates with more simulations.

#### 3.Required Sample Size N
```{r}
# For epsilon <= 0.03
epsilon_target <- 0.03

sigma_estimate <- sd(success)

N_required <- ceiling((z_score * sigma_estimate / epsilon_target) ^ 2)

N_required
```

**Comment:** To guarantee that the margin of error $\varepsilon \le 0.03$, we use the formula $\varepsilon = z \cdot \frac{\sigma}{\sqrt{N}}$.
Solving for $N$, we get $N \ge \left(\frac{z \cdot \sigma}{\varepsilon}\right)^2$.
We use our sample standard deviation as an estimate for $\sigma$ and $z = 1.96$ for 95% confidence.
The calculation shows that we need approximately $N \approx 850$ simulations to achieve the desired precision.
This makes sense: smaller margin of error requires more data - if we want our estimate to be more accurate (within $\pm 0.03$), we need to run more experiments.
Note that this is just an estimate based on our initial sample of $N=100$; the actual required $N$ might vary slightly.

#### 4.Histogram of Number of Errors
```{r}
# Count how many bits were corrupted in each message
num_errors <- rowSums(errors)

# Histogram
hist(num_errors,
     breaks = seq(-0.5, 7.5, by = 1),
     main = "Distribution of Number of Errors per Message",
     xlab = "Number of Errors (k)",
     ylab = "Frequency",
     col = "lightblue",
     right = FALSE)

# Bar plot
error_counts <- table(num_errors)
barplot(error_counts,
        main = "Distribution of Number of Errors",
        xlab = "Number of Errors (k)",
        ylab = "Frequency",
        col = "lightblue")
```

**Comment:** The histogram shows how many errors occurred in each 7-bit codeword during transmission.
Since each of the 7 bits can be corrupted independently with probability $p = 0.31$, the number of errors follows a **Binomial distribution** with parameters $n=7$ (number of bits) and $p=0.31$ (probability each bit is corrupted).
The expected number of errors is $7 \times 0.31 \approx 2.17$, so having 2 errors is most common, which matches what we observe in the histogram.

Crucially, the Hamming $[7,4]$ code can only correct messages with **0 or 1 error**.
It fails when **2 or more errors** occur.
At our error rate of $p=0.31$, the probability of getting 2 or more errors is approximately $1 - p^* = 1 - 0.309 \approx 0.69$ (about 69% of messages), which explains why the overall success rate $p^* \approx 0.31$ is relatively low.
The code works well only when bit error rates are much smaller.

**Summary:** In this task, we implemented and analyzed the $[7,4]$ Hamming code, which protects 4-bit messages from corruptions by encoding them into 7-bit codewords.
With our bit error probability of $p=0.31$ (team id=31), we found that the theoretical probability of successfully transmitting a message is $p^* = 0.309$ (about 31%).
Our simulation with $N=100$ gave an estimate of $\hat p = 0.27$, which is close to the theoretical value and falls within our 95% confidence interval $[0.183, 0.357]$.

We used the Central Limit Theorem to calculate this confidence interval with margin of error $\varepsilon = 0.087$.
To achieve a more precise estimate with $\varepsilon \le 0.03$, we determined that approximately **850 simulations** are needed.

The error distribution follows a **Binomial$(7, 0.31)$** pattern, with most codewords experiencing 2-3 bit errors.
Since the Hamming code can only correct single-bit errors (0 or 1 error), messages with 2 or more errorsâwhich occur in about 69% of transmissions at $p=0.31$âfail to be decoded correctly.
This explains the relatively low success rate of approximately 31%.
The Hamming code would be much more effective at lower bit error rates (e.g., $p < 0.1$), where single errors dominate and multiple errors are rare.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
lambda <- 1  # change this!
N <- 100     # change this!
mu <- N * lambda
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs,
     xlim = xlims,
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
nu1 <- 1  # change this!
K <- 1e3
n <- 5
sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- 0       # change this!
sigma <- 1    # change this!
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs,
     xlim = xlims,
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps**

**Do not forget to include several sentences summarizing your work and the conclusions you have made!**

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

**1.  In this part, we discuss independence of random variables and its moments: expectation and variance.**

1.1 Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

**Explanation:** The expectation operator $\mathbb{E}$ is linear, but not multiplicative for non-linear transformations. The function $f(x) = \frac{1}{x}$ is non-linear and convex (for positive $x$), so by Jensen's inequality: $\mathbb{E}(f(X)) \geq f(\mathbb{E}(X))$.

1.2 Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

```{r}
# Set parameters
mu <- id
sigma <- sqrt(id * 2 + 7)
N <- 100

# Generate random variables
X <- rnorm(N, mean = mu, sd = sigma)
Y <- 1 / X

# Calculate values
value1 <- 1 / mean(X)
value2 <- mean(Y)

value1
value2
```

**Comment:** The simulation shows that the two values are different, confirming what we explained above. We see that $\frac{1}{\overline{X}}$ (which is 1 divided by the average of $X$) is not equal to $\overline{Y}$ (the average of all $\frac{1}{X}$ values). The second one is larger because when $X$ has smaller values, $\frac{1}{X}$ becomes very large, which pulls up the average. So taking the average first and then dividing gives a different result than dividing first and then taking the average.

1.3 Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

```{r}
# Set parameters
lambda <- 2

# Generate random variables
X <- rexp(N, rate = lambda)
Y <- rexp(N, rate = lambda)
Z <- log(X) + 5

# Q-Q Plot: X vs Y
qqplot(X, Y,
       main = "Q-Q Plot: X vs Y",
       xlab = "Quantiles of X (Exponential, Î»=2)",
       ylab = "Quantiles of Y (Exponential, Î»=2)",
       pch = 19, col = "#1f77b4")
abline(0, 1, col = "red", lwd = 2, lty = 2) # Intercept = 0, Slope = 1 (x = y)
grid()

# Scatterplot: X vs Y
plot(X, Y,
     main = "Scatterplot: X vs Y",
     xlab = "X (Exponential, Î»=2)",
     ylab = "Y (Exponential, Î»=2)",
     pch = 19, col = "#2ca02c")
grid()

# Q-Q Plot: X vs Z
qqplot(X, Z,
       main = "Q-Q Plot: X vs Z",
       xlab = "Quantiles of X (Exponential, Î»=2)",
       ylab = "Quantiles of Z = log(X) + 5",
       pch = 19, col = "#ff7f0e")
abline(0, 1, col = "red", lwd = 2, lty = 2) # Intercept = 0, Slope = 1 (x = y)
grid()

# Scatterplot: X vs Z
plot(X, Z,
     main = "Scatterplot: X vs Z",
     xlab = "X (Exponential, Î»=2)",
     ylab = "Z = log(X) + 5",
     pch = 19, col = "#9467bd")
grid()
```

**Comment on X vs Y:** Both $X$ and $Y$ are generated randomly and independently from the same type of distribution (exponential). The Q-Q plot shows points close to a straight line, meaning they have the same distribution shape. The scatterplot shows random scattered points with no pattern, which means $X$ and $Y$ are independent - knowing one doesn't tell you anything about the other. They are similar in distribution but unrelated.

**Comment on X vs Z:** Here $Z$ is calculated directly from $X$ using the formula $Z = \log(X) + 5$, so they are completely related (dependent). The Q-Q plot doesn't follow a straight line because $Z$ has a different distribution type than $X$. The scatterplot shows a clear curved pattern - when $X$ increases, $Z$ also increases in a predictable way. This pair is dependent because $Z$ is computed from $X$.

**Summary:** $(X, Y)$ are independent with the same distribution type, while $(X, Z)$ are dependent with different distribution types.

```
------------------------------------------------------------------------
```

**2. You toss a fair coin three times and a random variable** $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost Â£$1$. They will receive Â£$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$? To answer this,

2.1 Explain what type of random variable is X:

-   Normally distributed

-   Binomially distributed YES

-   Poisson distributed

-   Uniformly distributed

**Explanation:** $X$ is *binomially distributed* because it counts the number of successes (Heads) in a fixed number of independent trials (3 coin tosses), where each trial has the same probability of success ($p = 0.5$). We write $X \sim \text{Binomial}(n=3, p=0.5)$.

2.2 What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

```{r}
# Set parameters
n_tosses <- 3
p_head <- 0.5
N <- 100

# Generate random variable
X <- rbinom(N, size = n_tosses, prob = p_head)

# Calculate values
sample_mean_X <- mean(X)
sample_var_X <- var(X)

# Calculate theoretical values
# E(X) = n * p
# Var(X) = n * p * (1 - p)
theoretical_mean_X <- n_tosses * p_head
theoretical_var_X <- n_tosses * p_head * (1 - p_head)

sample_mean_X
sample_var_X
theoretical_mean_X
theoretical_var_X
```

**Comment:** The simulation confirms the theoretical results. The sample mean is close to the theoretical expected value $\mathbb{E}(X) = np = 3 \times 0.5 = 1.5$ heads, and the sample variance is close to the theoretical variance $\text{Var}(X) = np(1-p) = 3 \times 0.5 \times 0.5 = 0.75$. Small differences exist due to random sampling with only $N=100$ simulations, but they become negligible with larger sample sizes.

2.3 What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

```{r}
# Generate random variable
Y <- 0.5 * X - 1

# Calculate values
sample_mean_Y <- mean(Y)
sample_var_Y <- var(Y)

# Calculate theoretical values
# E(aX + b) = aE(X) + b
# Var(aX + b) = a^2Var(X)
a <- 0.5
b <- -1

theoretical_mean_Y <- a * theoretical_mean_X + b
theoretical_var_Y <- a^2 * theoretical_var_X

sample_mean_Y
sample_var_Y
theoretical_mean_Y
theoretical_var_Y
```

**Comment:** The expected value of $Y$ is $\mathbb{E}(Y) = 0.5 \times 1.5 - 1 = -0.25$ pounds, meaning on average your friend loses Â£$0.25$ per round. This makes the game unfavorable for them. The variance of $Y$ is $\text{Var}(Y) = (0.5)^2 \times 0.75 = 0.1875$, which is smaller than $\text{Var}(X)$ because of the scaling factor $(0.5)^2$. The simulation results match the theoretical values well, confirming that linear transformations affect expectation and variance according to the formulas $\mathbb{E}(aX + b) = a\mathbb{E}(X) + b$ and $\text{Var}(aX + b) = a^2\text{Var}(X)$.

**Conclusion:** In this task, we explored key properties of random variables and their transformations. We demonstrated that expectation is not preserved under non-linear transformations like reciprocals, showing that $\mathbb{E}\left(\frac{1}{X}\right) \neq \frac{1}{\mathbb{E}(X)}$ due to Jensen's inequality. Through Q-Q plots and scatterplots, we distinguished between independent random variables ($X$ and $Y$) that share the same distribution, and dependent variables ($X$ and $Z$) with a deterministic relationship $Z = \log(X) + 5$. Finally, we analyzed a coin-tossing game using binomial distribution $X \sim \text{Binomial}(n=3, p=0.5)$, calculating expected values and variances for both the number of heads and the game payoff $Y = 0.5X - 1$. The simulations consistently matched theoretical predictions, confirming our understanding of expectation, variance, independence, and linear transformations of random variables.

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.
